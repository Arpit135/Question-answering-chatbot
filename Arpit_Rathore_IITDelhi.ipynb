{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch sentence-transformers\n",
    "# pip install PyPDF2\n",
    "# pip install sentence-transformers\n",
    "#!pip install openai==0.28\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "#import openai\n",
    "#from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/bladerunner/blade runner 2049.pdf\", \"rb\") as pdf_file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    text = ''\n",
    "\n",
    "    # Loop through each page and extract the text\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 1250\n",
    "part_size = len(text) // num_parts\n",
    "text_parts = []\n",
    "for i in range(num_parts):\n",
    "    start_index = i * part_size\n",
    "    end_index = (i + 1) * part_size if i < num_parts - 1 else len(text)\n",
    "    text_part = text[start_index:end_index]\n",
    "    text_parts.append(text_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "def evaluate(text_parts,my_query):  \n",
    "#     nlp = spacy.load(\"en_core_web_md\")\n",
    "#     # Define the query and convert it to a spaCy Doc object\n",
    "#     query = my_query\n",
    "#     query_doc = nlp(query)\n",
    "\n",
    "#     # Calculate semantic similarity scores between the query and each sentence\n",
    "#     scores = []\n",
    "#     for text_part in text_parts:\n",
    "#         sentence_doc = nlp(text_part)\n",
    "#         similarity_score = query_doc.similarity(sentence_doc)\n",
    "#         scores.append((text_part, similarity_score))\n",
    "\n",
    "#     # Sort sentences by similarity score (optional)\n",
    "#     scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     # Define the query\n",
    "    query = my_query\n",
    "\n",
    "    # Compute embedding for the query and the sentences\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    sentence_embeddings = model.encode(text_parts, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, sentence_embeddings)\n",
    "\n",
    "\n",
    "    # Create a list of tuples containing sentence and cosine similarity score\n",
    "    scores = [(text_parts[i], cosine_scores[0][i].item()) for i in range(len(text_parts))]\n",
    "\n",
    "    # Sort scores in decreasing order\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a dictionary to store total weighted score for each sentence\n",
    "    score_dict = {}\n",
    "    weight = 1\n",
    "    #for pairs, weight in zip([normalized_scores], [0.2, 0.6, 0.2]):\n",
    "    for text_part, score in scores:\n",
    "        if text_part not in score_dict:\n",
    "            score_dict[text_part] = 0\n",
    "        score_dict[text_part] += weight * score\n",
    "\n",
    "    # Now score_dict contains the weighted average score for each sentence\n",
    "    weighted_scores = list(score_dict.items())\n",
    "\n",
    "    # Sort scores in decreasing order\n",
    "    weighted_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    return weighted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\"  # You can choose larger models like flan-t5-large\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "question = \"Who is the writer?\"\n",
    "#question =  \"Explain the theme of the movie\"\n",
    "#question =  \"Who are the characters?\"\n",
    "#question =  \"Does the script pass the Bechdel test?\"\n",
    "weighted_scores = evaluate(text_parts, question)\n",
    "\n",
    "# Construct the context by combining the top-weighted scores\n",
    "context = \". \".join([score[0] for score in weighted_scores[:min(10,len(weighted_scores))]]) + \".\"\n",
    "\n",
    "# Create a prompt with the context and the question\n",
    "prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=500,  # Increase max_length for longer responses\n",
    "    temperature=1.1,  # Adjust temperature for more varied output\n",
    "    top_k=50,  # Control sampling diversity\n",
    "    top_p=0.95,  # Control the probability distribution for output tokens\n",
    "    num_return_sequences=1,  # Return multiple sequences if you want to select the best one\n",
    "    do_sample=True  # Enable sampling for varied outputs\n",
    ")\n",
    "\n",
    "# Decode the output to get the generated answer\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(context)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
